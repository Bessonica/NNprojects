{"cells":[{"cell_type":"markdown","metadata":{},"source":["\n","github url:  https://github.com/Bessonica/NNprojects\n","\n","# The problem:\n","  we must build NN that would understand what tweets are about real disaster and what is not.\n","\n","# Data:\n"," * train.csv\n"," * test.csv\n"," * sample_submission.csv\n"," \n","# Each sample of data consists of:\n","* text of tweet\n","* Key word for tweet\n","* location of tweet\n"," \n","# Columns:\n"," * id - a unique identifier for each tweet\n"," * text - the text of the tweet\n"," * location - the location the tweet was sent from (may be blank)\n"," * keyword - a particular keyword from the tweet (may be blank)\n"," * target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Import all needed libraries\n","\n","**matplotlib, seaborn for graph**"]},{"cell_type":"code","execution_count":69,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-06-23T18:53:34.834581Z","iopub.status.busy":"2022-06-23T18:53:34.833762Z","iopub.status.idle":"2022-06-23T18:53:34.847834Z","shell.execute_reply":"2022-06-23T18:53:34.846512Z","shell.execute_reply.started":"2022-06-23T18:53:34.834538Z"},"trusted":true},"outputs":[],"source":["import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt\n","\n","# seaborn =  data visualization library\n","import seaborn as sns\n","%matplotlib inline\n","\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"markdown","metadata":{},"source":["**NN packages**"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2022-06-23T18:53:34.852319Z","iopub.status.busy":"2022-06-23T18:53:34.851467Z","iopub.status.idle":"2022-06-23T18:53:34.860376Z","shell.execute_reply":"2022-06-23T18:53:34.859545Z","shell.execute_reply.started":"2022-06-23T18:53:34.852270Z"},"trusted":true},"outputs":[],"source":["\n","\n","import nltk \n","from nltk.corpus import stopwords\n","\n","import re \n","from sklearn.model_selection import train_test_split\n","\n","\n","from keras.preprocessing.text import Tokenizer \n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils.np_utils import to_categorical\n","\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding,GRU, LSTM, RNN\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","import keras.backend as K\n"]},{"cell_type":"markdown","metadata":{},"source":["# EDA\n","# Inspect, visualize, clean\n","\n"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2022-06-23T18:53:34.973208Z","iopub.status.busy":"2022-06-23T18:53:34.971887Z","iopub.status.idle":"2022-06-23T18:53:35.018498Z","shell.execute_reply":"2022-06-23T18:53:35.017297Z","shell.execute_reply.started":"2022-06-23T18:53:34.973152Z"},"trusted":true},"outputs":[],"source":["\n","train_data=pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n","train_data.head()\n","\n","test_data=pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n","test_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Visualize (What data we have?)\n","\n","how much tweets real disaster, and how much is not\n","\n"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2022-06-23T18:53:35.118881Z","iopub.status.busy":"2022-06-23T18:53:35.118300Z","iopub.status.idle":"2022-06-23T18:53:35.274743Z","shell.execute_reply":"2022-06-23T18:53:35.273916Z","shell.execute_reply.started":"2022-06-23T18:53:35.118847Z"},"trusted":true},"outputs":[],"source":["#values  0 = not disaster, 1 = real disaster\n","\n","\n","#count data based on \"target\" column\n","sns.countplot(train_data['target'])\n","\n","plt.title('No disaster:'+str(train_data.target.value_counts()[0])+'\\n'+\n","         'Real disaster:'+str(train_data.target.value_counts()[1]))\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["length of words in tweets"]},{"cell_type":"code","execution_count":73,"metadata":{"execution":{"iopub.execute_input":"2022-06-23T18:53:35.276752Z","iopub.status.busy":"2022-06-23T18:53:35.276275Z","iopub.status.idle":"2022-06-23T18:53:35.650836Z","shell.execute_reply":"2022-06-23T18:53:35.649992Z","shell.execute_reply.started":"2022-06-23T18:53:35.276721Z"},"trusted":true},"outputs":[],"source":["#take every tweet and split them based on space\n","#and count their length\n","\n","def words_len(arr, text):\n","    word_len = []\n","    for i in arr:\n","        word_len.append(len(i.split(' ')))\n","\n","    plt.figure(figsize=(12,6))\n","    sns.countplot(word_len)\n","    plt.xlabel(\"Lengths of words:\")\n","    plt.ylabel('How many:')\n","    plt.title(text)\n","    \n","    plt.show()\n","    \n","words_len(train_data['text'],\"length of words in train data set\")"]},{"cell_type":"markdown","metadata":{},"source":["# Clean data   \n","In our implementation of NN we dont need keyword and location column"]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2022-06-23T18:53:35.653844Z","iopub.status.busy":"2022-06-23T18:53:35.653133Z","iopub.status.idle":"2022-06-23T18:53:35.662464Z","shell.execute_reply":"2022-06-23T18:53:35.661358Z","shell.execute_reply.started":"2022-06-23T18:53:35.653798Z"},"trusted":true},"outputs":[],"source":["train_data.drop(['keyword','location'], axis=1, inplace=True)\n","test_data.drop(['keyword','location'], axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["# Clean data     \n","\n","lets think about what data is useless\n","* short words\n","* special symbols, such as    * http =   \n","* stopwords\n","* numbers\n","* non English letters\n","\n","Then normalize data\n","* make all lower case\n","\n"]},{"cell_type":"code","execution_count":75,"metadata":{"execution":{"iopub.execute_input":"2022-06-23T18:53:35.664666Z","iopub.status.busy":"2022-06-23T18:53:35.664238Z","iopub.status.idle":"2022-06-23T18:53:35.676677Z","shell.execute_reply":"2022-06-23T18:53:35.675818Z","shell.execute_reply.started":"2022-06-23T18:53:35.664611Z"},"trusted":true},"outputs":[],"source":["#we use nltk to get stopwords\n","#stopwords are category of words that search engine ignore, because they useless\n","#we can take this data to our advantage\n","\n","# swords=set(stopwords.words('english'))\n","\n","stopWords=set(stopwords.words('english'))\n","\n","def clear_txt(text):\n","    #to lower case\n","    h_str = text.lower()\n","    \n","    h_str = re.sub(r'(http|https)?\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b','',h_str)\n","    \n","    h_str = re.sub(r'\\{[^)]*\\}', '', h_str)\n","    h_str = re.sub(r'\\([^)]*\\)', '', h_str)\n","    \n","    h_str = re.sub('[^a-zA-Z]', ' ', h_str)\n","    \n","    tokens = [w for w in h_str.split() if not w in stopWords] \n","    \n","    res = []\n","    \n","    #take out all words shorter than 4\n","    for i in tokens:\n","        if len(i) >=4:\n","            res.append(i)\n","    return (\" \".join(res)).strip()"]},{"cell_type":"code","execution_count":76,"metadata":{"execution":{"iopub.execute_input":"2022-06-23T18:53:35.680129Z","iopub.status.busy":"2022-06-23T18:53:35.679403Z","iopub.status.idle":"2022-06-23T18:53:35.971233Z","shell.execute_reply":"2022-06-23T18:53:35.969977Z","shell.execute_reply.started":"2022-06-23T18:53:35.680066Z"},"trusted":true},"outputs":[],"source":["clean_trainData = []\n","for i in train_data['text']:\n","    clean_trainData.append(clear_txt(i))"]},{"cell_type":"markdown","metadata":{},"source":["**Lets look on some example of our work**"]},{"cell_type":"code","execution_count":77,"metadata":{"execution":{"iopub.execute_input":"2022-06-23T18:53:35.973449Z","iopub.status.busy":"2022-06-23T18:53:35.973131Z","iopub.status.idle":"2022-06-23T18:53:35.979274Z","shell.execute_reply":"2022-06-23T18:53:35.978445Z","shell.execute_reply.started":"2022-06-23T18:53:35.973421Z"},"trusted":true},"outputs":[],"source":["for i in range (3):\n","    print(\"Before: \", train_data['text'][i])\n","    print(\"After: \", clean_trainData[i])\n","    print()\n","\n","# print(\"Before: \", train_data['text'][7])\n","# print(\"After: \", clean_trainData[7])"]},{"cell_type":"code","execution_count":78,"metadata":{"execution":{"iopub.execute_input":"2022-06-23T18:53:35.981008Z","iopub.status.busy":"2022-06-23T18:53:35.980234Z","iopub.status.idle":"2022-06-23T18:53:36.111487Z","shell.execute_reply":"2022-06-23T18:53:36.110302Z","shell.execute_reply.started":"2022-06-23T18:53:35.980968Z"},"trusted":true},"outputs":[],"source":["clean_testData = []\n","for i in test_data['text']:\n","    clean_testData.append(clear_txt(i))"]},{"cell_type":"markdown","metadata":{},"source":["# Visualise Result\n","\n","In data we left only important to us words. And and their len distribute like this:"]},{"cell_type":"code","execution_count":79,"metadata":{"execution":{"iopub.execute_input":"2022-06-23T18:53:36.114278Z","iopub.status.busy":"2022-06-23T18:53:36.113483Z","iopub.status.idle":"2022-06-23T18:53:36.606039Z","shell.execute_reply":"2022-06-23T18:53:36.605196Z","shell.execute_reply.started":"2022-06-23T18:53:36.114244Z"},"trusted":true},"outputs":[],"source":["words_len(clean_trainData,\"Train data set\")\n","words_len(clean_testData,\"Test data set\")"]},{"cell_type":"markdown","metadata":{},"source":["# Prepare data \n","\n","Divade data into train and validation set (4:1)."]},{"cell_type":"code","execution_count":80,"metadata":{"execution":{"iopub.execute_input":"2022-06-23T18:53:36.609124Z","iopub.status.busy":"2022-06-23T18:53:36.608416Z","iopub.status.idle":"2022-06-23T18:53:36.621214Z","shell.execute_reply":"2022-06-23T18:53:36.620008Z","shell.execute_reply.started":"2022-06-23T18:53:36.609060Z"},"trusted":true},"outputs":[],"source":["#train_test_split  splits array into random train and validation subsets \n","X_train,X_valid,y_train,y_valid = train_test_split(clean_trainData, train_data['target'], test_size = 0.2, random_state = 40)\n","\n","\n","print(\"Check data, did we do it right?\")\n","print(\"It should be same size\")\n","print(f\"Train size: {len(X_train)}, {len(y_train)}\")\n","print(f\"Validation size: {len(X_valid)}, {len(y_valid)}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Time to tokenize tweets and create vocablurary\n","tokenize - separate text into words "]},{"cell_type":"code","execution_count":81,"metadata":{"execution":{"iopub.execute_input":"2022-06-23T18:53:36.622566Z","iopub.status.busy":"2022-06-23T18:53:36.622291Z","iopub.status.idle":"2022-06-23T18:53:36.955148Z","shell.execute_reply":"2022-06-23T18:53:36.953914Z","shell.execute_reply.started":"2022-06-23T18:53:36.622539Z"},"trusted":true},"outputs":[],"source":["#tokenizer fuction is gonna help tokenize text\n","\n","#plot above has shown that max length is 20\n","\n","\n","max_len = 20\n","\n","tokenizer=Tokenizer(oov_token='<OOV>')\n","tokenizer.fit_on_texts(X_train)\n","\n","X_train=tokenizer.texts_to_sequences(X_train)\n","X_valid=tokenizer.texts_to_sequences(X_valid)\n","\n","X_test=tokenizer.texts_to_sequences(clean_testData)\n","\n","\n","\n","# ensures they are same length\n","X_train=pad_sequences(X_train,maxlen=max_len,padding='post')\n","X_valid=pad_sequences(X_valid,maxlen=max_len,padding='post')\n","\n","X_test=pad_sequences(X_test,maxlen=max_len,padding='post')\n","\n","\n","\n","\n","\n","vocabluary = len(tokenizer.word_index) + 1\n","\n","\n","print(\"Vocabluary size \")\n","print(vocabluary)"]},{"cell_type":"markdown","metadata":{},"source":["# categorizing data"]},{"cell_type":"code","execution_count":82,"metadata":{"execution":{"iopub.execute_input":"2022-06-23T18:53:36.957650Z","iopub.status.busy":"2022-06-23T18:53:36.957319Z","iopub.status.idle":"2022-06-23T18:53:36.964361Z","shell.execute_reply":"2022-06-23T18:53:36.963565Z","shell.execute_reply.started":"2022-06-23T18:53:36.957611Z"},"trusted":true},"outputs":[],"source":["\n","\n","y_train=to_categorical(y_train,num_classes=2)\n","y_valid=to_categorical(y_valid,num_classes=2)\n","\n","    \n","    \n","print(\"data shape\")\n","print(y_train.shape)\n","print(y_valid.shape)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Model architecture\n","\n","after preparing data we can start designing our model architecture\n","\n","We gonna create **multiLayer LSTM**\n","\n","\n","\n","Lets add 4 LSTM layer with size 350,150,50.\n","Butch size now 200. We have greater losses, but the quality of education must increase.\n","\n","Other hyper parametrs are same."]},{"cell_type":"code","execution_count":83,"metadata":{"execution":{"iopub.execute_input":"2022-06-23T18:53:36.965900Z","iopub.status.busy":"2022-06-23T18:53:36.965439Z","iopub.status.idle":"2022-06-23T18:53:37.417227Z","shell.execute_reply":"2022-06-23T18:53:37.416161Z","shell.execute_reply.started":"2022-06-23T18:53:36.965871Z"},"trusted":true},"outputs":[],"source":["\n","\n","model=Sequential()\n","model.add(Embedding(vocabluary,100,input_length=max_len,trainable=True,mask_zero=True))\n","model.add(LSTM(350,dropout=0.1,recurrent_dropout=0.2,return_sequences=True))\n","model.add(LSTM(150,dropout=0.1,recurrent_dropout=0.2,return_sequences=True))\n","model.add(LSTM(50,dropout=0.1,recurrent_dropout=0.2))\n","model.add(Dense(64,activation='relu'))\n","model.add(Dense(2,activation='sigmoid'))\n","model.summary()\n","\n","model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])\n"]},{"cell_type":"code","execution_count":84,"metadata":{"execution":{"iopub.execute_input":"2022-06-23T18:53:37.419725Z","iopub.status.busy":"2022-06-23T18:53:37.419407Z","iopub.status.idle":"2022-06-23T19:07:24.366329Z","shell.execute_reply":"2022-06-23T19:07:24.365199Z","shell.execute_reply.started":"2022-06-23T18:53:37.419687Z"},"trusted":true},"outputs":[],"source":["h=model.fit(x=np.array(X_train),y=np.array(y_train),batch_size=200,epochs=30,\n","          validation_data=(np.array(X_valid),np.array(y_valid)))\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Lets visualize result\n","\n","\n","accuracy is 0.9\n","validation loss is around 1.2\n","\n"]},{"cell_type":"code","execution_count":85,"metadata":{"execution":{"iopub.execute_input":"2022-06-23T19:07:24.369950Z","iopub.status.busy":"2022-06-23T19:07:24.369564Z","iopub.status.idle":"2022-06-23T19:07:24.566606Z","shell.execute_reply":"2022-06-23T19:07:24.564989Z","shell.execute_reply.started":"2022-06-23T19:07:24.369917Z"},"trusted":true},"outputs":[],"source":["plt.plot(h.history['val_loss'],'r',label='val_loss')\n","plt.plot(h.history['loss'],'g',label='train_loss')\n","plt.legend()"]},{"cell_type":"code","execution_count":86,"metadata":{"execution":{"iopub.execute_input":"2022-06-23T19:07:24.568762Z","iopub.status.busy":"2022-06-23T19:07:24.568068Z","iopub.status.idle":"2022-06-23T19:07:24.758986Z","shell.execute_reply":"2022-06-23T19:07:24.757790Z","shell.execute_reply.started":"2022-06-23T19:07:24.568718Z"},"trusted":true},"outputs":[],"source":["plt.plot(h.history['val_acc'],'b',label='val_acc')\n","plt.plot(h.history['acc'],'y',label='train_acc')\n","plt.legend()"]},{"cell_type":"markdown","metadata":{},"source":["# Final submition\n"]},{"cell_type":"code","execution_count":87,"metadata":{"execution":{"iopub.execute_input":"2022-06-23T19:07:24.761220Z","iopub.status.busy":"2022-06-23T19:07:24.760581Z","iopub.status.idle":"2022-06-23T19:07:35.591797Z","shell.execute_reply":"2022-06-23T19:07:35.590535Z","shell.execute_reply.started":"2022-06-23T19:07:24.761178Z"},"trusted":true},"outputs":[],"source":["predict_help = model.predict(X_test)\n","prediction = [0 if i[0]>=0.5 else 1 for i in predict_help]\n","test_data['target'] = prediction\n","\n","sub = test_data[['id', 'target']]\n","print(\"Starting to write csv\")\n","sub.to_csv('Submission.csv', index=False)\n","print(\"Ended writing csv\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
