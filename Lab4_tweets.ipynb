{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\ngithub url:  https://github.com/Bessonica/NNprojects\n\n# The problem:\n  we must build NN that would understand what tweets are about real disaster and what is not.\n\n# Data:\n * train.csv\n * test.csv\n * sample_submission.csv\n \n# Each sample of data consists of:\n* text of tweet\n* Key word for tweet\n* location of tweet\n \n# Columns:\n * id - a unique identifier for each tweet\n * text - the text of the tweet\n * location - the location the tweet was sent from (may be blank)\n * keyword - a particular keyword from the tweet (may be blank)\n * target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Import all needed libraries\n\n**matplotlib, seaborn for graph**","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n\n# seaborn =  data visualization library\nimport seaborn as sns\n%matplotlib inline\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-23T18:53:34.833762Z","iopub.execute_input":"2022-06-23T18:53:34.834581Z","iopub.status.idle":"2022-06-23T18:53:34.847834Z","shell.execute_reply.started":"2022-06-23T18:53:34.834538Z","shell.execute_reply":"2022-06-23T18:53:34.846512Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"**NN packages**","metadata":{}},{"cell_type":"code","source":"\n\nimport nltk \nfrom nltk.corpus import stopwords\n\nimport re \nfrom sklearn.model_selection import train_test_split\n\n\nfrom keras.preprocessing.text import Tokenizer \nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding,GRU, LSTM, RNN\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nimport keras.backend as K\n","metadata":{"execution":{"iopub.status.busy":"2022-06-23T18:53:34.851467Z","iopub.execute_input":"2022-06-23T18:53:34.852319Z","iopub.status.idle":"2022-06-23T18:53:34.860376Z","shell.execute_reply.started":"2022-06-23T18:53:34.852270Z","shell.execute_reply":"2022-06-23T18:53:34.859545Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"# EDA\n# Inspect, visualize, clean\n\n","metadata":{}},{"cell_type":"code","source":"\ntrain_data=pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntrain_data.head()\n\ntest_data=pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ntest_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T18:53:34.971887Z","iopub.execute_input":"2022-06-23T18:53:34.973208Z","iopub.status.idle":"2022-06-23T18:53:35.018498Z","shell.execute_reply.started":"2022-06-23T18:53:34.973152Z","shell.execute_reply":"2022-06-23T18:53:35.017297Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize (What data we have?)\n\nhow much tweets real disaster, and how much is not\n\n","metadata":{}},{"cell_type":"code","source":"#values  0 = not disaster, 1 = real disaster\n\n\n#count data based on \"target\" column\nsns.countplot(train_data['target'])\n\nplt.title('No disaster:'+str(train_data.target.value_counts()[0])+'\\n'+\n         'Real disaster:'+str(train_data.target.value_counts()[1]))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T18:53:35.118300Z","iopub.execute_input":"2022-06-23T18:53:35.118881Z","iopub.status.idle":"2022-06-23T18:53:35.274743Z","shell.execute_reply.started":"2022-06-23T18:53:35.118847Z","shell.execute_reply":"2022-06-23T18:53:35.273916Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"length of words in tweets","metadata":{}},{"cell_type":"code","source":"#take every tweet and split them based on space\n#and count their length\n\ndef words_len(arr, text):\n    word_len = []\n    for i in arr:\n        word_len.append(len(i.split(' ')))\n\n    plt.figure(figsize=(12,6))\n    sns.countplot(word_len)\n    plt.xlabel(\"Lengths of words:\")\n    plt.ylabel('How many:')\n    plt.title(text)\n    \n    plt.show()\n    \nwords_len(train_data['text'],\"length of words in train data set\")","metadata":{"execution":{"iopub.status.busy":"2022-06-23T18:53:35.276275Z","iopub.execute_input":"2022-06-23T18:53:35.276752Z","iopub.status.idle":"2022-06-23T18:53:35.650836Z","shell.execute_reply.started":"2022-06-23T18:53:35.276721Z","shell.execute_reply":"2022-06-23T18:53:35.649992Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"# Clean data   \nIn our implementation of NN we dont need keyword and location column","metadata":{}},{"cell_type":"code","source":"train_data.drop(['keyword','location'], axis=1, inplace=True)\ntest_data.drop(['keyword','location'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T18:53:35.653133Z","iopub.execute_input":"2022-06-23T18:53:35.653844Z","iopub.status.idle":"2022-06-23T18:53:35.662464Z","shell.execute_reply.started":"2022-06-23T18:53:35.653798Z","shell.execute_reply":"2022-06-23T18:53:35.661358Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"# Clean data     \n\nlets think about what data is useless\n* short words\n* special symbols, such as    * http =   \n* stopwords\n* numbers\n* non English letters\n\nThen normalize data\n* make all lower case\n\n","metadata":{}},{"cell_type":"code","source":"#we use nltk to get stopwords\n#stopwords are category of words that search engine ignore, because they useless\n#we can take this data to our advantage\n\n# swords=set(stopwords.words('english'))\n\nstopWords=set(stopwords.words('english'))\n\ndef clear_txt(text):\n    #to lower case\n    h_str = text.lower()\n    \n    h_str = re.sub(r'(http|https)?\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b','',h_str)\n    \n    h_str = re.sub(r'\\{[^)]*\\}', '', h_str)\n    h_str = re.sub(r'\\([^)]*\\)', '', h_str)\n    \n    h_str = re.sub('[^a-zA-Z]', ' ', h_str)\n    \n    tokens = [w for w in h_str.split() if not w in stopWords] \n    \n    res = []\n    \n    #take out all words shorter than 4\n    for i in tokens:\n        if len(i) >=4:\n            res.append(i)\n    return (\" \".join(res)).strip()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T18:53:35.664238Z","iopub.execute_input":"2022-06-23T18:53:35.664666Z","iopub.status.idle":"2022-06-23T18:53:35.676677Z","shell.execute_reply.started":"2022-06-23T18:53:35.664611Z","shell.execute_reply":"2022-06-23T18:53:35.675818Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"clean_trainData = []\nfor i in train_data['text']:\n    clean_trainData.append(clear_txt(i))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T18:53:35.679403Z","iopub.execute_input":"2022-06-23T18:53:35.680129Z","iopub.status.idle":"2022-06-23T18:53:35.971233Z","shell.execute_reply.started":"2022-06-23T18:53:35.680066Z","shell.execute_reply":"2022-06-23T18:53:35.969977Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"**Lets look on some example of our work**","metadata":{}},{"cell_type":"code","source":"for i in range (3):\n    print(\"Before: \", train_data['text'][i])\n    print(\"After: \", clean_trainData[i])\n    print()\n\n# print(\"Before: \", train_data['text'][7])\n# print(\"After: \", clean_trainData[7])","metadata":{"execution":{"iopub.status.busy":"2022-06-23T18:53:35.973131Z","iopub.execute_input":"2022-06-23T18:53:35.973449Z","iopub.status.idle":"2022-06-23T18:53:35.979274Z","shell.execute_reply.started":"2022-06-23T18:53:35.973421Z","shell.execute_reply":"2022-06-23T18:53:35.978445Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"clean_testData = []\nfor i in test_data['text']:\n    clean_testData.append(clear_txt(i))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T18:53:35.980234Z","iopub.execute_input":"2022-06-23T18:53:35.981008Z","iopub.status.idle":"2022-06-23T18:53:36.111487Z","shell.execute_reply.started":"2022-06-23T18:53:35.980968Z","shell.execute_reply":"2022-06-23T18:53:36.110302Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"# Visualise Result\n\nIn data we left only important to us words. And and their len distribute like this:","metadata":{}},{"cell_type":"code","source":"words_len(clean_trainData,\"Train data set\")\nwords_len(clean_testData,\"Test data set\")","metadata":{"execution":{"iopub.status.busy":"2022-06-23T18:53:36.113483Z","iopub.execute_input":"2022-06-23T18:53:36.114278Z","iopub.status.idle":"2022-06-23T18:53:36.606039Z","shell.execute_reply.started":"2022-06-23T18:53:36.114244Z","shell.execute_reply":"2022-06-23T18:53:36.605196Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":"# Prepare data \n\nDivade data into train and validation set (4:1).","metadata":{}},{"cell_type":"code","source":"#train_test_split  splits array into random train and validation subsets \nX_train,X_valid,y_train,y_valid = train_test_split(clean_trainData, train_data['target'], test_size = 0.2, random_state = 40)\n\n\nprint(\"Check data, did we do it right?\")\nprint(\"It should be same size\")\nprint(f\"Train size: {len(X_train)}, {len(y_train)}\")\nprint(f\"Validation size: {len(X_valid)}, {len(y_valid)}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-23T18:53:36.608416Z","iopub.execute_input":"2022-06-23T18:53:36.609124Z","iopub.status.idle":"2022-06-23T18:53:36.621214Z","shell.execute_reply.started":"2022-06-23T18:53:36.609060Z","shell.execute_reply":"2022-06-23T18:53:36.620008Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":"# Time to tokenize tweets and create vocablurary\ntokenize - separate text into words ","metadata":{}},{"cell_type":"code","source":"#tokenizer fuction is gonna help tokenize text\n\n#plot above has shown that max length is 20\n\n\nmax_len = 20\n\ntokenizer=Tokenizer(oov_token='<OOV>')\ntokenizer.fit_on_texts(X_train)\n\nX_train=tokenizer.texts_to_sequences(X_train)\nX_valid=tokenizer.texts_to_sequences(X_valid)\n\nX_test=tokenizer.texts_to_sequences(clean_testData)\n\n\n\n# ensures they are same length\nX_train=pad_sequences(X_train,maxlen=max_len,padding='post')\nX_valid=pad_sequences(X_valid,maxlen=max_len,padding='post')\n\nX_test=pad_sequences(X_test,maxlen=max_len,padding='post')\n\n\n\n\n\nvocabluary = len(tokenizer.word_index) + 1\n\n\nprint(\"Vocabluary size \")\nprint(vocabluary)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T18:53:36.622291Z","iopub.execute_input":"2022-06-23T18:53:36.622566Z","iopub.status.idle":"2022-06-23T18:53:36.955148Z","shell.execute_reply.started":"2022-06-23T18:53:36.622539Z","shell.execute_reply":"2022-06-23T18:53:36.953914Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"# categorizing data","metadata":{}},{"cell_type":"code","source":"\n\ny_train=to_categorical(y_train,num_classes=2)\ny_valid=to_categorical(y_valid,num_classes=2)\n\n    \n    \nprint(\"data shape\")\nprint(y_train.shape)\nprint(y_valid.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T18:53:36.957319Z","iopub.execute_input":"2022-06-23T18:53:36.957650Z","iopub.status.idle":"2022-06-23T18:53:36.964361Z","shell.execute_reply.started":"2022-06-23T18:53:36.957611Z","shell.execute_reply":"2022-06-23T18:53:36.963565Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":"**Compile our model, using most popular and simple optimazer for RNN - ADAM.**\n\nAnd also divade data into batches with size 650. If this value is lower, then the growth of loss is too rapid. You can put more, but it is important not to cross the line when 30 epochs are not enough for full-fledged learning.","metadata":{}},{"cell_type":"markdown","source":"# Model architecture\n\nafter preparing data we can start designing our model architecture\n\nWe gonna create **multiLayer LSTM**\n\n\n\nLets add 4 LSTM layer with size 350,150,50.\nButch size now 200. We have greater losses, but the quality of education must increase.\n\nOther hyper parametrs are same.","metadata":{}},{"cell_type":"code","source":"\n\nmodel=Sequential()\nmodel.add(Embedding(vocabluary,100,input_length=max_len,trainable=True,mask_zero=True))\nmodel.add(LSTM(350,dropout=0.1,recurrent_dropout=0.2,return_sequences=True))\nmodel.add(LSTM(150,dropout=0.1,recurrent_dropout=0.2,return_sequences=True))\nmodel.add(LSTM(50,dropout=0.1,recurrent_dropout=0.2))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dense(2,activation='sigmoid'))\nmodel.summary()\n\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])\n","metadata":{"execution":{"iopub.status.busy":"2022-06-23T18:53:36.965439Z","iopub.execute_input":"2022-06-23T18:53:36.965900Z","iopub.status.idle":"2022-06-23T18:53:37.417227Z","shell.execute_reply.started":"2022-06-23T18:53:36.965871Z","shell.execute_reply":"2022-06-23T18:53:37.416161Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"h=model.fit(x=np.array(X_train),y=np.array(y_train),batch_size=200,epochs=30,\n          validation_data=(np.array(X_valid),np.array(y_valid)))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-23T18:53:37.419407Z","iopub.execute_input":"2022-06-23T18:53:37.419725Z","iopub.status.idle":"2022-06-23T19:07:24.366329Z","shell.execute_reply.started":"2022-06-23T18:53:37.419687Z","shell.execute_reply":"2022-06-23T19:07:24.365199Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":"# Lets visualize result\n\n\naccuracy is 0.9\nvalidation loss is around 1.2\n\n","metadata":{}},{"cell_type":"code","source":"plt.plot(h.history['val_loss'],'r',label='val_loss')\nplt.plot(h.history['loss'],'g',label='train_loss')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T19:07:24.369564Z","iopub.execute_input":"2022-06-23T19:07:24.369950Z","iopub.status.idle":"2022-06-23T19:07:24.566606Z","shell.execute_reply.started":"2022-06-23T19:07:24.369917Z","shell.execute_reply":"2022-06-23T19:07:24.564989Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"plt.plot(h.history['val_acc'],'b',label='val_acc')\nplt.plot(h.history['acc'],'y',label='train_acc')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T19:07:24.568068Z","iopub.execute_input":"2022-06-23T19:07:24.568762Z","iopub.status.idle":"2022-06-23T19:07:24.758986Z","shell.execute_reply.started":"2022-06-23T19:07:24.568718Z","shell.execute_reply":"2022-06-23T19:07:24.757790Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"markdown","source":"# Final submition\n","metadata":{}},{"cell_type":"code","source":"predict_help = model.predict(X_test)\nprediction = [0 if i[0]>=0.5 else 1 for i in predict_help]\ntest_data['target'] = prediction\n\nsub = test_data[['id', 'target']]\nprint(\"Starting to write csv\")\nsub.to_csv('Submission.csv', index=False)\nprint(\"Ended writing csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-23T19:07:24.760581Z","iopub.execute_input":"2022-06-23T19:07:24.761220Z","iopub.status.idle":"2022-06-23T19:07:35.591797Z","shell.execute_reply.started":"2022-06-23T19:07:24.761178Z","shell.execute_reply":"2022-06-23T19:07:35.590535Z"},"trusted":true},"execution_count":87,"outputs":[]}]}